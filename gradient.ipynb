{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0846ba72-472c-4089-9e90-75bd5e154f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello dear!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello dear!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e14c69-fe30-463b-96ab-67712ee03fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bashorat (training dan avval) 4 soat o'qilganda: 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w= 1.26 loss= 4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w= 1.45 loss= 2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w= 1.6 loss= 1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w= 1.7 loss= 0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w= 1.78 loss= 0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w= 1.84 loss= 0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w= 1.88 loss= 0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w= 1.91 loss= 0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w= 1.93 loss= 0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w= 1.95 loss= 0.02\n",
      "Bashorat (training dan keyin) 4 saot o'qilganda:  7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "# Pythonda \n",
    "# Training Data(O'rgatishdagi ma'lumotlar)\n",
    "x_soat = [1.0, 2.0, 3.0]\n",
    "y_baho = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  #w uchun dastalbki taxminiy qiymat\n",
    "\n",
    "\n",
    "# (Modelimiz)To'g'ri hisoblash uchun funksiya\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Xatolik (Loss) ning funkisyasi\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "\n",
    "# Gradient uchun funksiya\n",
    "def gradient(x, y):  # d_loss/d_w\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "\n",
    "# Training dan avval\n",
    "print(\"Bashorat (training dan avval)\",  \"4 soat o'qilganda:\", forward(4))\n",
    "\n",
    "# Training zanjiri (loop)\n",
    "learning_rate =0.01\n",
    "for epoch in range(10):\n",
    "    for x_hb_qiym, y_hb_qiym in zip(x_soat, y_baho):\n",
    "        # Hosilani hisoblash\n",
    "        # w ning qiymatini yangilash\n",
    "        # xatolikni hisoblab progressni chop qilish\n",
    "        grad = gradient(x_hb_qiym, y_hb_qiym)\n",
    "        w = w - learning_rate * grad\n",
    "        print(\"\\tgrad: \", x_hb_qiym, y_hb_qiym, round(grad, 2))\n",
    "        l = loss(x_hb_qiym, y_hb_qiym)\n",
    "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
    "\n",
    "# Traningdan so'ng\n",
    "print(\"Bashorat (training dan keyin)\",  \"4 saot o'qilganda: \", forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd041d5-4e11-4757-a777-29a795de2308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bashorat (training dan avval) 4 soat o'qilganda: tensor([4.], grad_fn=<MulBackward0>)\n",
      "\tgrad:  1.0 2.0 -2.000\n",
      "\tgrad:  2.0 4.0 -7.840\n",
      "\tgrad:  3.0 6.0 -16.229\n",
      "Epoch: 0 | Loss: 7.315943717956543\n",
      "\tgrad:  1.0 2.0 -1.479\n",
      "\tgrad:  2.0 4.0 -5.796\n",
      "\tgrad:  3.0 6.0 -11.998\n",
      "Epoch: 1 | Loss: 3.9987640380859375\n",
      "\tgrad:  1.0 2.0 -1.093\n",
      "\tgrad:  2.0 4.0 -4.285\n",
      "\tgrad:  3.0 6.0 -8.870\n",
      "Epoch: 2 | Loss: 2.1856532096862793\n",
      "\tgrad:  1.0 2.0 -0.808\n",
      "\tgrad:  2.0 4.0 -3.168\n",
      "\tgrad:  3.0 6.0 -6.558\n",
      "Epoch: 3 | Loss: 1.1946394443511963\n",
      "\tgrad:  1.0 2.0 -0.598\n",
      "\tgrad:  2.0 4.0 -2.342\n",
      "\tgrad:  3.0 6.0 -4.848\n",
      "Epoch: 4 | Loss: 0.6529689431190491\n",
      "\tgrad:  1.0 2.0 -0.442\n",
      "\tgrad:  2.0 4.0 -1.732\n",
      "\tgrad:  3.0 6.0 -3.584\n",
      "Epoch: 5 | Loss: 0.35690122842788696\n",
      "\tgrad:  1.0 2.0 -0.327\n",
      "\tgrad:  2.0 4.0 -1.280\n",
      "\tgrad:  3.0 6.0 -2.650\n",
      "Epoch: 6 | Loss: 0.195076122879982\n",
      "\tgrad:  1.0 2.0 -0.241\n",
      "\tgrad:  2.0 4.0 -0.946\n",
      "\tgrad:  3.0 6.0 -1.959\n",
      "Epoch: 7 | Loss: 0.10662525147199631\n",
      "\tgrad:  1.0 2.0 -0.179\n",
      "\tgrad:  2.0 4.0 -0.700\n",
      "\tgrad:  3.0 6.0 -1.448\n",
      "Epoch: 8 | Loss: 0.0582793727517128\n",
      "\tgrad:  1.0 2.0 -0.132\n",
      "\tgrad:  2.0 4.0 -0.517\n",
      "\tgrad:  3.0 6.0 -1.071\n",
      "Epoch: 9 | Loss: 0.03185431286692619\n",
      "Bashorat (training dan keyin) 4 saot o'qilganda:  tensor([7.8049], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Kerakli kutubxonalrni chaqirib olish\n",
    "import torch\n",
    "\n",
    "x_soat = [1.0, 2.0, 3.0]\n",
    "y_baho = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.tensor([1.0], requires_grad=True) #Taxminiy qiymat\n",
    "\n",
    "# (Modelimiz)To'g'ri hisoblash uchun funksiya\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "\n",
    "# Xatolik (Loss) ning funkisyasi\n",
    "def loss(y_pred, y_val):\n",
    "    return (y_pred - y_val) ** 2          \n",
    "\n",
    "# Training dan avval\n",
    "print(\"Bashorat (training dan avval)\",  \"4 soat o'qilganda:\", forward(4))\n",
    "\n",
    "# Training zanjiri (loop)\n",
    "learning_rate = 0.01\n",
    "for epoch in range(10):\n",
    "    for x_hb_qiym, y_hb_qiym in zip(x_soat, y_baho):\n",
    "        y_pred = forward(x_hb_qiym) # 1) Forward hisoblash\n",
    "        l = loss(y_pred, y_hb_qiym) # 2) Loss ni hisoblash\n",
    "        l.backward() # 3) backward hisoblash\n",
    "        print(\"\\tgrad: \", x_hb_qiym, y_hb_qiym, '{:.3f}'.format(w.grad.item()))\n",
    "        w.data = w.data - learning_rate * w.grad.item()  #W ning qiymatini yangilash\n",
    "\n",
    "        # w ning qiymattini yangilagach, nolga tenglashtirish\n",
    "        w.grad.data.zero_()\n",
    "\n",
    "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
    "\n",
    "# Traningdan so'ng\n",
    "print(\"Bashorat (training dan keyin)\",  \"4 saot o'qilganda: \", forward(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c992690-3573-4b18-bc0f-f142a8252800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 48.11091995239258 \n",
      "Epoch: 1 | Loss: 21.712602615356445 \n",
      "Epoch: 2 | Loss: 9.956584930419922 \n",
      "Epoch: 3 | Loss: 4.718960762023926 \n",
      "Epoch: 4 | Loss: 2.3831992149353027 \n",
      "Epoch: 5 | Loss: 1.3393263816833496 \n",
      "Epoch: 6 | Loss: 0.8706220984458923 \n",
      "Epoch: 7 | Loss: 0.6580262184143066 \n",
      "Epoch: 8 | Loss: 0.5594971179962158 \n",
      "Epoch: 9 | Loss: 0.5118033289909363 \n",
      "Epoch: 10 | Loss: 0.4867960512638092 \n",
      "Epoch: 11 | Loss: 0.4719420373439789 \n",
      "Epoch: 12 | Loss: 0.46166113018989563 \n",
      "Epoch: 13 | Loss: 0.45346882939338684 \n",
      "Epoch: 14 | Loss: 0.4462585151195526 \n",
      "Epoch: 15 | Loss: 0.43953636288642883 \n",
      "Epoch: 16 | Loss: 0.4330819845199585 \n",
      "Epoch: 17 | Loss: 0.42679715156555176 \n",
      "Epoch: 18 | Loss: 0.42063623666763306 \n",
      "Epoch: 19 | Loss: 0.4145784378051758 \n",
      "Epoch: 20 | Loss: 0.40861499309539795 \n",
      "Epoch: 21 | Loss: 0.40274032950401306 \n",
      "Epoch: 22 | Loss: 0.396950900554657 \n",
      "Epoch: 23 | Loss: 0.3912459909915924 \n",
      "Epoch: 24 | Loss: 0.38562318682670593 \n",
      "Epoch: 25 | Loss: 0.3800807595252991 \n",
      "Epoch: 26 | Loss: 0.3746183514595032 \n",
      "Epoch: 27 | Loss: 0.3692342936992645 \n",
      "Epoch: 28 | Loss: 0.363927960395813 \n",
      "Epoch: 29 | Loss: 0.358697772026062 \n",
      "Epoch: 30 | Loss: 0.3535428047180176 \n",
      "Epoch: 31 | Loss: 0.34846195578575134 \n",
      "Epoch: 32 | Loss: 0.34345364570617676 \n",
      "Epoch: 33 | Loss: 0.3385176658630371 \n",
      "Epoch: 34 | Loss: 0.33365291357040405 \n",
      "Epoch: 35 | Loss: 0.32885774970054626 \n",
      "Epoch: 36 | Loss: 0.3241317868232727 \n",
      "Epoch: 37 | Loss: 0.31947317719459534 \n",
      "Epoch: 38 | Loss: 0.31488192081451416 \n",
      "Epoch: 39 | Loss: 0.3103564977645874 \n",
      "Epoch: 40 | Loss: 0.3058963418006897 \n",
      "Epoch: 41 | Loss: 0.3015001714229584 \n",
      "Epoch: 42 | Loss: 0.29716697335243225 \n",
      "Epoch: 43 | Loss: 0.2928960919380188 \n",
      "Epoch: 44 | Loss: 0.2886868119239807 \n",
      "Epoch: 45 | Loss: 0.2845379412174225 \n",
      "Epoch: 46 | Loss: 0.2804487943649292 \n",
      "Epoch: 47 | Loss: 0.27641817927360535 \n",
      "Epoch: 48 | Loss: 0.27244535088539124 \n",
      "Epoch: 49 | Loss: 0.2685301899909973 \n",
      "Epoch: 50 | Loss: 0.2646712362766266 \n",
      "Epoch: 51 | Loss: 0.26086732745170593 \n",
      "Epoch: 52 | Loss: 0.25711819529533386 \n",
      "Epoch: 53 | Loss: 0.25342291593551636 \n",
      "Epoch: 54 | Loss: 0.24978087842464447 \n",
      "Epoch: 55 | Loss: 0.24619115889072418 \n",
      "Epoch: 56 | Loss: 0.2426530122756958 \n",
      "Epoch: 57 | Loss: 0.2391657531261444 \n",
      "Epoch: 58 | Loss: 0.2357286810874939 \n",
      "Epoch: 59 | Loss: 0.23234066367149353 \n",
      "Epoch: 60 | Loss: 0.22900153696537018 \n",
      "Epoch: 61 | Loss: 0.22571057081222534 \n",
      "Epoch: 62 | Loss: 0.2224666178226471 \n",
      "Epoch: 63 | Loss: 0.21926945447921753 \n",
      "Epoch: 64 | Loss: 0.21611806750297546 \n",
      "Epoch: 65 | Loss: 0.2130122184753418 \n",
      "Epoch: 66 | Loss: 0.20995089411735535 \n",
      "Epoch: 67 | Loss: 0.20693372189998627 \n",
      "Epoch: 68 | Loss: 0.20395949482917786 \n",
      "Epoch: 69 | Loss: 0.2010282278060913 \n",
      "Epoch: 70 | Loss: 0.19813914597034454 \n",
      "Epoch: 71 | Loss: 0.19529162347316742 \n",
      "Epoch: 72 | Loss: 0.19248493015766144 \n",
      "Epoch: 73 | Loss: 0.18971888720989227 \n",
      "Epoch: 74 | Loss: 0.18699216842651367 \n",
      "Epoch: 75 | Loss: 0.18430495262145996 \n",
      "Epoch: 76 | Loss: 0.18165603280067444 \n",
      "Epoch: 77 | Loss: 0.17904525995254517 \n",
      "Epoch: 78 | Loss: 0.1764720231294632 \n",
      "Epoch: 79 | Loss: 0.17393600940704346 \n",
      "Epoch: 80 | Loss: 0.17143605649471283 \n",
      "Epoch: 81 | Loss: 0.16897255182266235 \n",
      "Epoch: 82 | Loss: 0.16654406487941742 \n",
      "Epoch: 83 | Loss: 0.1641504168510437 \n",
      "Epoch: 84 | Loss: 0.16179145872592926 \n",
      "Epoch: 85 | Loss: 0.15946634113788605 \n",
      "Epoch: 86 | Loss: 0.15717445313930511 \n",
      "Epoch: 87 | Loss: 0.15491549670696259 \n",
      "Epoch: 88 | Loss: 0.15268923342227936 \n",
      "Epoch: 89 | Loss: 0.15049490332603455 \n",
      "Epoch: 90 | Loss: 0.14833205938339233 \n",
      "Epoch: 91 | Loss: 0.14620016515254974 \n",
      "Epoch: 92 | Loss: 0.14409920573234558 \n",
      "Epoch: 93 | Loss: 0.14202798902988434 \n",
      "Epoch: 94 | Loss: 0.1399870067834854 \n",
      "Epoch: 95 | Loss: 0.1379752904176712 \n",
      "Epoch: 96 | Loss: 0.13599207997322083 \n",
      "Epoch: 97 | Loss: 0.1340378373861313 \n",
      "Epoch: 98 | Loss: 0.13211137056350708 \n",
      "Epoch: 99 | Loss: 0.13021287322044373 \n",
      "Epoch: 100 | Loss: 0.12834148108959198 \n",
      "Epoch: 101 | Loss: 0.12649688124656677 \n",
      "Epoch: 102 | Loss: 0.12467905879020691 \n",
      "Epoch: 103 | Loss: 0.12288711220026016 \n",
      "Epoch: 104 | Loss: 0.12112119793891907 \n",
      "Epoch: 105 | Loss: 0.11938031017780304 \n",
      "Epoch: 106 | Loss: 0.11766475439071655 \n",
      "Epoch: 107 | Loss: 0.11597371101379395 \n",
      "Epoch: 108 | Loss: 0.11430700123310089 \n",
      "Epoch: 109 | Loss: 0.11266428232192993 \n",
      "Epoch: 110 | Loss: 0.1110449731349945 \n",
      "Epoch: 111 | Loss: 0.10944926738739014 \n",
      "Epoch: 112 | Loss: 0.10787632316350937 \n",
      "Epoch: 113 | Loss: 0.10632598400115967 \n",
      "Epoch: 114 | Loss: 0.1047978550195694 \n",
      "Epoch: 115 | Loss: 0.10329167544841766 \n",
      "Epoch: 116 | Loss: 0.10180730372667313 \n",
      "Epoch: 117 | Loss: 0.10034417361021042 \n",
      "Epoch: 118 | Loss: 0.09890199452638626 \n",
      "Epoch: 119 | Loss: 0.09748073667287827 \n",
      "Epoch: 120 | Loss: 0.09607971459627151 \n",
      "Epoch: 121 | Loss: 0.09469898790121078 \n",
      "Epoch: 122 | Loss: 0.09333791583776474 \n",
      "Epoch: 123 | Loss: 0.09199646860361099 \n",
      "Epoch: 124 | Loss: 0.09067444503307343 \n",
      "Epoch: 125 | Loss: 0.08937133848667145 \n",
      "Epoch: 126 | Loss: 0.08808676153421402 \n",
      "Epoch: 127 | Loss: 0.08682088553905487 \n",
      "Epoch: 128 | Loss: 0.08557316660881042 \n",
      "Epoch: 129 | Loss: 0.08434341847896576 \n",
      "Epoch: 130 | Loss: 0.08313114196062088 \n",
      "Epoch: 131 | Loss: 0.08193644136190414 \n",
      "Epoch: 132 | Loss: 0.08075879514217377 \n",
      "Epoch: 133 | Loss: 0.07959823310375214 \n",
      "Epoch: 134 | Loss: 0.07845421880483627 \n",
      "Epoch: 135 | Loss: 0.07732686400413513 \n",
      "Epoch: 136 | Loss: 0.07621541619300842 \n",
      "Epoch: 137 | Loss: 0.07512004673480988 \n",
      "Epoch: 138 | Loss: 0.07404059916734695 \n",
      "Epoch: 139 | Loss: 0.07297638058662415 \n",
      "Epoch: 140 | Loss: 0.07192764431238174 \n",
      "Epoch: 141 | Loss: 0.07089392840862274 \n",
      "Epoch: 142 | Loss: 0.06987515091896057 \n",
      "Epoch: 143 | Loss: 0.06887096166610718 \n",
      "Epoch: 144 | Loss: 0.0678810402750969 \n",
      "Epoch: 145 | Loss: 0.06690565496683121 \n",
      "Epoch: 146 | Loss: 0.06594409048557281 \n",
      "Epoch: 147 | Loss: 0.06499618291854858 \n",
      "Epoch: 148 | Loss: 0.06406223773956299 \n",
      "Epoch: 149 | Loss: 0.06314157694578171 \n",
      "Epoch: 150 | Loss: 0.06223408505320549 \n",
      "Epoch: 151 | Loss: 0.06133957952260971 \n",
      "Epoch: 152 | Loss: 0.06045810878276825 \n",
      "Epoch: 153 | Loss: 0.05958925560116768 \n",
      "Epoch: 154 | Loss: 0.05873281508684158 \n",
      "Epoch: 155 | Loss: 0.05788872018456459 \n",
      "Epoch: 156 | Loss: 0.05705685541033745 \n",
      "Epoch: 157 | Loss: 0.05623690038919449 \n",
      "Epoch: 158 | Loss: 0.055428601801395416 \n",
      "Epoch: 159 | Loss: 0.05463195964694023 \n",
      "Epoch: 160 | Loss: 0.0538468137383461 \n",
      "Epoch: 161 | Loss: 0.05307299271225929 \n",
      "Epoch: 162 | Loss: 0.05231018364429474 \n",
      "Epoch: 163 | Loss: 0.05155843868851662 \n",
      "Epoch: 164 | Loss: 0.050817541778087616 \n",
      "Epoch: 165 | Loss: 0.050087131559848785 \n",
      "Epoch: 166 | Loss: 0.049367327243089676 \n",
      "Epoch: 167 | Loss: 0.04865790158510208 \n",
      "Epoch: 168 | Loss: 0.04795847833156586 \n",
      "Epoch: 169 | Loss: 0.04726933687925339 \n",
      "Epoch: 170 | Loss: 0.046589914709329605 \n",
      "Epoch: 171 | Loss: 0.045920372009277344 \n",
      "Epoch: 172 | Loss: 0.045260485261678696 \n",
      "Epoch: 173 | Loss: 0.04461003094911575 \n",
      "Epoch: 174 | Loss: 0.04396886005997658 \n",
      "Epoch: 175 | Loss: 0.04333703592419624 \n",
      "Epoch: 176 | Loss: 0.04271417111158371 \n",
      "Epoch: 177 | Loss: 0.042100273072719574 \n",
      "Epoch: 178 | Loss: 0.041495271027088165 \n",
      "Epoch: 179 | Loss: 0.040898941457271576 \n",
      "Epoch: 180 | Loss: 0.04031108319759369 \n",
      "Epoch: 181 | Loss: 0.03973175585269928 \n",
      "Epoch: 182 | Loss: 0.03916066139936447 \n",
      "Epoch: 183 | Loss: 0.03859803080558777 \n",
      "Epoch: 184 | Loss: 0.0380433164536953 \n",
      "Epoch: 185 | Loss: 0.037496455013751984 \n",
      "Epoch: 186 | Loss: 0.03695773333311081 \n",
      "Epoch: 187 | Loss: 0.03642650693655014 \n",
      "Epoch: 188 | Loss: 0.03590305894613266 \n",
      "Epoch: 189 | Loss: 0.035386987030506134 \n",
      "Epoch: 190 | Loss: 0.034878406673669815 \n",
      "Epoch: 191 | Loss: 0.03437718376517296 \n",
      "Epoch: 192 | Loss: 0.03388310968875885 \n",
      "Epoch: 193 | Loss: 0.03339628130197525 \n",
      "Epoch: 194 | Loss: 0.03291621059179306 \n",
      "Epoch: 195 | Loss: 0.032443199306726456 \n",
      "Epoch: 196 | Loss: 0.03197688236832619 \n",
      "Epoch: 197 | Loss: 0.031517352908849716 \n",
      "Epoch: 198 | Loss: 0.03106442466378212 \n",
      "Epoch: 199 | Loss: 0.03061787039041519 \n",
      "Epoch: 200 | Loss: 0.03017789125442505 \n",
      "Epoch: 201 | Loss: 0.02974425069987774 \n",
      "Epoch: 202 | Loss: 0.02931671217083931 \n",
      "Epoch: 203 | Loss: 0.028895467519760132 \n",
      "Epoch: 204 | Loss: 0.028480106964707375 \n",
      "Epoch: 205 | Loss: 0.028070801869034767 \n",
      "Epoch: 206 | Loss: 0.027667496353387833 \n",
      "Epoch: 207 | Loss: 0.027269799262285233 \n",
      "Epoch: 208 | Loss: 0.0268778707832098 \n",
      "Epoch: 209 | Loss: 0.026491615921258926 \n",
      "Epoch: 210 | Loss: 0.026110898703336716 \n",
      "Epoch: 211 | Loss: 0.02573564648628235 \n",
      "Epoch: 212 | Loss: 0.0253656804561615 \n",
      "Epoch: 213 | Loss: 0.02500126138329506 \n",
      "Epoch: 214 | Loss: 0.02464188262820244 \n",
      "Epoch: 215 | Loss: 0.024287741631269455 \n",
      "Epoch: 216 | Loss: 0.023938719183206558 \n",
      "Epoch: 217 | Loss: 0.023594675585627556 \n",
      "Epoch: 218 | Loss: 0.02325558103621006 \n",
      "Epoch: 219 | Loss: 0.022921334952116013 \n",
      "Epoch: 220 | Loss: 0.022591955959796906 \n",
      "Epoch: 221 | Loss: 0.022267259657382965 \n",
      "Epoch: 222 | Loss: 0.021947205066680908 \n",
      "Epoch: 223 | Loss: 0.021631859242916107 \n",
      "Epoch: 224 | Loss: 0.021320879459381104 \n",
      "Epoch: 225 | Loss: 0.021014533936977386 \n",
      "Epoch: 226 | Loss: 0.020712587982416153 \n",
      "Epoch: 227 | Loss: 0.02041488327085972 \n",
      "Epoch: 228 | Loss: 0.020121492445468903 \n",
      "Epoch: 229 | Loss: 0.019832327961921692 \n",
      "Epoch: 230 | Loss: 0.01954728364944458 \n",
      "Epoch: 231 | Loss: 0.019266318529844284 \n",
      "Epoch: 232 | Loss: 0.01898948661983013 \n",
      "Epoch: 233 | Loss: 0.018716542050242424 \n",
      "Epoch: 234 | Loss: 0.018447572365403175 \n",
      "Epoch: 235 | Loss: 0.018182439729571342 \n",
      "Epoch: 236 | Loss: 0.017921078950166702 \n",
      "Epoch: 237 | Loss: 0.017663516104221344 \n",
      "Epoch: 238 | Loss: 0.01740972325205803 \n",
      "Epoch: 239 | Loss: 0.017159517854452133 \n",
      "Epoch: 240 | Loss: 0.016912875697016716 \n",
      "Epoch: 241 | Loss: 0.016669830307364464 \n",
      "Epoch: 242 | Loss: 0.01643025130033493 \n",
      "Epoch: 243 | Loss: 0.01619412936270237 \n",
      "Epoch: 244 | Loss: 0.015961412340402603 \n",
      "Epoch: 245 | Loss: 0.015732022002339363 \n",
      "Epoch: 246 | Loss: 0.015505898743867874 \n",
      "Epoch: 247 | Loss: 0.015283109620213509 \n",
      "Epoch: 248 | Loss: 0.015063452534377575 \n",
      "Epoch: 249 | Loss: 0.014846928417682648 \n",
      "Epoch: 250 | Loss: 0.01463359035551548 \n",
      "Epoch: 251 | Loss: 0.014423277229070663 \n",
      "Epoch: 252 | Loss: 0.014215969480574131 \n",
      "Epoch: 253 | Loss: 0.014011669903993607 \n",
      "Epoch: 254 | Loss: 0.01381026953458786 \n",
      "Epoch: 255 | Loss: 0.013611792586743832 \n",
      "Epoch: 256 | Loss: 0.013416226953268051 \n",
      "Epoch: 257 | Loss: 0.013223361223936081 \n",
      "Epoch: 258 | Loss: 0.013033331371843815 \n",
      "Epoch: 259 | Loss: 0.012846021912992 \n",
      "Epoch: 260 | Loss: 0.012661428190767765 \n",
      "Epoch: 261 | Loss: 0.012479469180107117 \n",
      "Epoch: 262 | Loss: 0.0123000992462039 \n",
      "Epoch: 263 | Loss: 0.01212329976260662 \n",
      "Epoch: 264 | Loss: 0.011949090287089348 \n",
      "Epoch: 265 | Loss: 0.011777378618717194 \n",
      "Epoch: 266 | Loss: 0.011608093976974487 \n",
      "Epoch: 267 | Loss: 0.011441253125667572 \n",
      "Epoch: 268 | Loss: 0.01127682439982891 \n",
      "Epoch: 269 | Loss: 0.011114799417555332 \n",
      "Epoch: 270 | Loss: 0.010955005884170532 \n",
      "Epoch: 271 | Loss: 0.010797590017318726 \n",
      "Epoch: 272 | Loss: 0.010642433539032936 \n",
      "Epoch: 273 | Loss: 0.010489460080862045 \n",
      "Epoch: 274 | Loss: 0.010338672436773777 \n",
      "Epoch: 275 | Loss: 0.010190177708864212 \n",
      "Epoch: 276 | Loss: 0.010043695569038391 \n",
      "Epoch: 277 | Loss: 0.00989934429526329 \n",
      "Epoch: 278 | Loss: 0.009757060557603836 \n",
      "Epoch: 279 | Loss: 0.009616832248866558 \n",
      "Epoch: 280 | Loss: 0.009478638879954815 \n",
      "Epoch: 281 | Loss: 0.00934242270886898 \n",
      "Epoch: 282 | Loss: 0.00920813623815775 \n",
      "Epoch: 283 | Loss: 0.009075818583369255 \n",
      "Epoch: 284 | Loss: 0.0089454036206007 \n",
      "Epoch: 285 | Loss: 0.008816790767014027 \n",
      "Epoch: 286 | Loss: 0.008690128102898598 \n",
      "Epoch: 287 | Loss: 0.008565220981836319 \n",
      "Epoch: 288 | Loss: 0.0084421681240201 \n",
      "Epoch: 289 | Loss: 0.008320813067257404 \n",
      "Epoch: 290 | Loss: 0.008201188407838345 \n",
      "Epoch: 291 | Loss: 0.008083372376859188 \n",
      "Epoch: 292 | Loss: 0.007967149838805199 \n",
      "Epoch: 293 | Loss: 0.007852654904127121 \n",
      "Epoch: 294 | Loss: 0.007739818189293146 \n",
      "Epoch: 295 | Loss: 0.007628600113093853 \n",
      "Epoch: 296 | Loss: 0.007518943399190903 \n",
      "Epoch: 297 | Loss: 0.007410865742713213 \n",
      "Epoch: 298 | Loss: 0.007304396014660597 \n",
      "Epoch: 299 | Loss: 0.007199387066066265 \n",
      "Epoch: 300 | Loss: 0.007095946930348873 \n",
      "Epoch: 301 | Loss: 0.006993974559009075 \n",
      "Epoch: 302 | Loss: 0.006893459241837263 \n",
      "Epoch: 303 | Loss: 0.006794395856559277 \n",
      "Epoch: 304 | Loss: 0.006696729455143213 \n",
      "Epoch: 305 | Loss: 0.0066005028784275055 \n",
      "Epoch: 306 | Loss: 0.006505642086267471 \n",
      "Epoch: 307 | Loss: 0.006412134505808353 \n",
      "Epoch: 308 | Loss: 0.006319955922663212 \n",
      "Epoch: 309 | Loss: 0.006229118909686804 \n",
      "Epoch: 310 | Loss: 0.00613964069634676 \n",
      "Epoch: 311 | Loss: 0.006051361560821533 \n",
      "Epoch: 312 | Loss: 0.0059644137509167194 \n",
      "Epoch: 313 | Loss: 0.005878683645278215 \n",
      "Epoch: 314 | Loss: 0.005794202908873558 \n",
      "Epoch: 315 | Loss: 0.0057109566405415535 \n",
      "Epoch: 316 | Loss: 0.005628840997815132 \n",
      "Epoch: 317 | Loss: 0.005547964945435524 \n",
      "Epoch: 318 | Loss: 0.005468255840241909 \n",
      "Epoch: 319 | Loss: 0.005389660131186247 \n",
      "Epoch: 320 | Loss: 0.005312163382768631 \n",
      "Epoch: 321 | Loss: 0.0052358126267790794 \n",
      "Epoch: 322 | Loss: 0.005160595290362835 \n",
      "Epoch: 323 | Loss: 0.005086452700197697 \n",
      "Epoch: 324 | Loss: 0.005013332702219486 \n",
      "Epoch: 325 | Loss: 0.00494126882404089 \n",
      "Epoch: 326 | Loss: 0.004870266653597355 \n",
      "Epoch: 327 | Loss: 0.004800298251211643 \n",
      "Epoch: 328 | Loss: 0.004731307737529278 \n",
      "Epoch: 329 | Loss: 0.004663288593292236 \n",
      "Epoch: 330 | Loss: 0.004596271552145481 \n",
      "Epoch: 331 | Loss: 0.004530200734734535 \n",
      "Epoch: 332 | Loss: 0.004465128295123577 \n",
      "Epoch: 333 | Loss: 0.004400936421006918 \n",
      "Epoch: 334 | Loss: 0.004337691236287355 \n",
      "Epoch: 335 | Loss: 0.004275337792932987 \n",
      "Epoch: 336 | Loss: 0.004213911015540361 \n",
      "Epoch: 337 | Loss: 0.004153348505496979 \n",
      "Epoch: 338 | Loss: 0.004093662835657597 \n",
      "Epoch: 339 | Loss: 0.0040348307229578495 \n",
      "Epoch: 340 | Loss: 0.003976834937930107 \n",
      "Epoch: 341 | Loss: 0.003919688519090414 \n",
      "Epoch: 342 | Loss: 0.003863371443003416 \n",
      "Epoch: 343 | Loss: 0.0038078315556049347 \n",
      "Epoch: 344 | Loss: 0.003753092372789979 \n",
      "Epoch: 345 | Loss: 0.0036991701927036047 \n",
      "Epoch: 346 | Loss: 0.003646003780886531 \n",
      "Epoch: 347 | Loss: 0.0035936194472014904 \n",
      "Epoch: 348 | Loss: 0.0035419738851487637 \n",
      "Epoch: 349 | Loss: 0.0034910596441477537 \n",
      "Epoch: 350 | Loss: 0.0034408685751259327 \n",
      "Epoch: 351 | Loss: 0.0033914472442120314 \n",
      "Epoch: 352 | Loss: 0.0033426894806325436 \n",
      "Epoch: 353 | Loss: 0.003294658148661256 \n",
      "Epoch: 354 | Loss: 0.003247296204790473 \n",
      "Epoch: 355 | Loss: 0.0032006418332457542 \n",
      "Epoch: 356 | Loss: 0.003154642414301634 \n",
      "Epoch: 357 | Loss: 0.003109299810603261 \n",
      "Epoch: 358 | Loss: 0.003064610529690981 \n",
      "Epoch: 359 | Loss: 0.0030205613002181053 \n",
      "Epoch: 360 | Loss: 0.002977157710120082 \n",
      "Epoch: 361 | Loss: 0.002934354590252042 \n",
      "Epoch: 362 | Loss: 0.002892189659178257 \n",
      "Epoch: 363 | Loss: 0.0028506312519311905 \n",
      "Epoch: 364 | Loss: 0.0028096940368413925 \n",
      "Epoch: 365 | Loss: 0.002769303973764181 \n",
      "Epoch: 366 | Loss: 0.0027295099571347237 \n",
      "Epoch: 367 | Loss: 0.002690242137759924 \n",
      "Epoch: 368 | Loss: 0.002651594113558531 \n",
      "Epoch: 369 | Loss: 0.0026134932413697243 \n",
      "Epoch: 370 | Loss: 0.0025759399868547916 \n",
      "Epoch: 371 | Loss: 0.0025389129295945168 \n",
      "Epoch: 372 | Loss: 0.002502415794879198 \n",
      "Epoch: 373 | Loss: 0.0024664399679750204 \n",
      "Epoch: 374 | Loss: 0.0024310112930834293 \n",
      "Epoch: 375 | Loss: 0.002396086696535349 \n",
      "Epoch: 376 | Loss: 0.002361631952226162 \n",
      "Epoch: 377 | Loss: 0.002327696420252323 \n",
      "Epoch: 378 | Loss: 0.002294231904670596 \n",
      "Epoch: 379 | Loss: 0.0022612586617469788 \n",
      "Epoch: 380 | Loss: 0.002228757133707404 \n",
      "Epoch: 381 | Loss: 0.0021967499051243067 \n",
      "Epoch: 382 | Loss: 0.0021651554852724075 \n",
      "Epoch: 383 | Loss: 0.002134050242602825 \n",
      "Epoch: 384 | Loss: 0.002103391569107771 \n",
      "Epoch: 385 | Loss: 0.0020731508266180754 \n",
      "Epoch: 386 | Loss: 0.002043368062004447 \n",
      "Epoch: 387 | Loss: 0.002013993915170431 \n",
      "Epoch: 388 | Loss: 0.0019850628450512886 \n",
      "Epoch: 389 | Loss: 0.00195653666742146 \n",
      "Epoch: 390 | Loss: 0.0019284212030470371 \n",
      "Epoch: 391 | Loss: 0.0019007069058716297 \n",
      "Epoch: 392 | Loss: 0.0018733744509518147 \n",
      "Epoch: 393 | Loss: 0.0018464471213519573 \n",
      "Epoch: 394 | Loss: 0.0018199010519310832 \n",
      "Epoch: 395 | Loss: 0.0017937514930963516 \n",
      "Epoch: 396 | Loss: 0.0017679843585938215 \n",
      "Epoch: 397 | Loss: 0.0017425757832825184 \n",
      "Epoch: 398 | Loss: 0.0017175270477309823 \n",
      "Epoch: 399 | Loss: 0.0016928368713706732 \n",
      "Epoch: 400 | Loss: 0.0016685246955603361 \n",
      "Epoch: 401 | Loss: 0.0016445440705865622 \n",
      "Epoch: 402 | Loss: 0.0016208916204050183 \n",
      "Epoch: 403 | Loss: 0.0015976103022694588 \n",
      "Epoch: 404 | Loss: 0.0015746613498777151 \n",
      "Epoch: 405 | Loss: 0.001552036264911294 \n",
      "Epoch: 406 | Loss: 0.0015297115314751863 \n",
      "Epoch: 407 | Loss: 0.0015077335992828012 \n",
      "Epoch: 408 | Loss: 0.001486080465838313 \n",
      "Epoch: 409 | Loss: 0.0014646885683760047 \n",
      "Epoch: 410 | Loss: 0.0014436573255807161 \n",
      "Epoch: 411 | Loss: 0.0014229185180738568 \n",
      "Epoch: 412 | Loss: 0.0014024515403434634 \n",
      "Epoch: 413 | Loss: 0.0013823144836351275 \n",
      "Epoch: 414 | Loss: 0.0013624411076307297 \n",
      "Epoch: 415 | Loss: 0.0013428779784590006 \n",
      "Epoch: 416 | Loss: 0.0013235576916486025 \n",
      "Epoch: 417 | Loss: 0.0013045286759734154 \n",
      "Epoch: 418 | Loss: 0.0012857811525464058 \n",
      "Epoch: 419 | Loss: 0.001267310231924057 \n",
      "Epoch: 420 | Loss: 0.0012490899534896016 \n",
      "Epoch: 421 | Loss: 0.0012311424361541867 \n",
      "Epoch: 422 | Loss: 0.001213462557643652 \n",
      "Epoch: 423 | Loss: 0.0011960109695792198 \n",
      "Epoch: 424 | Loss: 0.001178837614133954 \n",
      "Epoch: 425 | Loss: 0.001161877647973597 \n",
      "Epoch: 426 | Loss: 0.001145190093666315 \n",
      "Epoch: 427 | Loss: 0.0011287208180874586 \n",
      "Epoch: 428 | Loss: 0.0011125183664262295 \n",
      "Epoch: 429 | Loss: 0.0010965216206386685 \n",
      "Epoch: 430 | Loss: 0.001080764806829393 \n",
      "Epoch: 431 | Loss: 0.001065222779288888 \n",
      "Epoch: 432 | Loss: 0.0010499082272872329 \n",
      "Epoch: 433 | Loss: 0.001034812885336578 \n",
      "Epoch: 434 | Loss: 0.001019949559122324 \n",
      "Epoch: 435 | Loss: 0.0010052950819954276 \n",
      "Epoch: 436 | Loss: 0.000990839907899499 \n",
      "Epoch: 437 | Loss: 0.0009766148868948221 \n",
      "Epoch: 438 | Loss: 0.0009625739185139537 \n",
      "Epoch: 439 | Loss: 0.0009487348725087941 \n",
      "Epoch: 440 | Loss: 0.0009351081098429859 \n",
      "Epoch: 441 | Loss: 0.0009216733742505312 \n",
      "Epoch: 442 | Loss: 0.0009084218181669712 \n",
      "Epoch: 443 | Loss: 0.0008953618817031384 \n",
      "Epoch: 444 | Loss: 0.0008824983378872275 \n",
      "Epoch: 445 | Loss: 0.0008698051096871495 \n",
      "Epoch: 446 | Loss: 0.0008573042578063905 \n",
      "Epoch: 447 | Loss: 0.000844983384013176 \n",
      "Epoch: 448 | Loss: 0.0008328508120030165 \n",
      "Epoch: 449 | Loss: 0.0008208808721974492 \n",
      "Epoch: 450 | Loss: 0.0008090737392194569 \n",
      "Epoch: 451 | Loss: 0.0007974357577040792 \n",
      "Epoch: 452 | Loss: 0.0007859898032620549 \n",
      "Epoch: 453 | Loss: 0.0007747008930891752 \n",
      "Epoch: 454 | Loss: 0.0007635547663085163 \n",
      "Epoch: 455 | Loss: 0.0007525882683694363 \n",
      "Epoch: 456 | Loss: 0.0007417803280986845 \n",
      "Epoch: 457 | Loss: 0.0007311181398108602 \n",
      "Epoch: 458 | Loss: 0.0007205939618870616 \n",
      "Epoch: 459 | Loss: 0.000710240681655705 \n",
      "Epoch: 460 | Loss: 0.0007000407204031944 \n",
      "Epoch: 461 | Loss: 0.0006899741711094975 \n",
      "Epoch: 462 | Loss: 0.0006800683331675828 \n",
      "Epoch: 463 | Loss: 0.0006702914834022522 \n",
      "Epoch: 464 | Loss: 0.0006606510723941028 \n",
      "Epoch: 465 | Loss: 0.0006511591491289437 \n",
      "Epoch: 466 | Loss: 0.0006418022094294429 \n",
      "Epoch: 467 | Loss: 0.0006325787981040776 \n",
      "Epoch: 468 | Loss: 0.000623496132902801 \n",
      "Epoch: 469 | Loss: 0.0006145239458419383 \n",
      "Epoch: 470 | Loss: 0.000605705427005887 \n",
      "Epoch: 471 | Loss: 0.0005969861522316933 \n",
      "Epoch: 472 | Loss: 0.0005884175188839436 \n",
      "Epoch: 473 | Loss: 0.0005799542996101081 \n",
      "Epoch: 474 | Loss: 0.000571626063901931 \n",
      "Epoch: 475 | Loss: 0.0005634048720821738 \n",
      "Epoch: 476 | Loss: 0.000555310514755547 \n",
      "Epoch: 477 | Loss: 0.0005473290802910924 \n",
      "Epoch: 478 | Loss: 0.0005394729087129235 \n",
      "Epoch: 479 | Loss: 0.0005317070172168314 \n",
      "Epoch: 480 | Loss: 0.0005240706959739327 \n",
      "Epoch: 481 | Loss: 0.0005165398470126092 \n",
      "Epoch: 482 | Loss: 0.000509121164213866 \n",
      "Epoch: 483 | Loss: 0.0005017926450818777 \n",
      "Epoch: 484 | Loss: 0.0004945871187373996 \n",
      "Epoch: 485 | Loss: 0.0004874708829447627 \n",
      "Epoch: 486 | Loss: 0.00048046483425423503 \n",
      "Epoch: 487 | Loss: 0.00047356897266581655 \n",
      "Epoch: 488 | Loss: 0.0004667543980758637 \n",
      "Epoch: 489 | Loss: 0.0004600477986969054 \n",
      "Epoch: 490 | Loss: 0.0004534478939604014 \n",
      "Epoch: 491 | Loss: 0.00044693012023344636 \n",
      "Epoch: 492 | Loss: 0.00044050594442524016 \n",
      "Epoch: 493 | Loss: 0.0004341705935075879 \n",
      "Epoch: 494 | Loss: 0.0004279268905520439 \n",
      "Epoch: 495 | Loss: 0.00042178452713415027 \n",
      "Epoch: 496 | Loss: 0.00041572091868147254 \n",
      "Epoch: 497 | Loss: 0.00040975186857394874 \n",
      "Epoch: 498 | Loss: 0.0004038575279992074 \n",
      "Epoch: 499 | Loss: 0.00039805343840271235 \n",
      "Bashorat (training dan keyin),  4 saot o'qilganda: 7.977064609527588\n"
     ]
    }
   ],
   "source": [
    "#Kerakli kutubxonalarni chaqirib olish \n",
    "import torch \n",
    "import numpy as np\n",
    "#Ma'lumotlarni tensor ko'rinishida yuklab olish\n",
    "x_soat = torch.Tensor([[1.0],\n",
    "                       [2.0],\n",
    "                       [3.0]])\n",
    "y_baho = torch.Tensor([[2.0],\n",
    "                       [4.0],\n",
    "                       [6.0]])\n",
    "\n",
    "#(1) Class yordamida model qurib olish --> \"Model\"\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        #Bu yerda torch.nn.Module bu yerda super class(Pytorch)\n",
    "        super().__init__()\n",
    "        #torch.nn.Linear(#kirish, #chiqish) chiziqli model\n",
    "        self.linear = torch.nn.Linear(1,1) #1ta kirish & 1ta chiqish\n",
    "    #Metod yordamida to'g'ri hisoblash funksiyasini kiritamiz(forward pass)    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "#Bizning model    \n",
    "model=Model()\n",
    "# print(model)\n",
    "#(2) Loss va optimizer larni tanlab olish \n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#(3) Training(3.1),   Backward(3.2), Step(3.3)\n",
    "#(3.1)-->Training\n",
    "for epoch in range(500):    #Epochlar soni 500\n",
    "    y_pred = model(x_soat)\n",
    "    #Loss|||xatolikni hisoblash va chop qilish\n",
    "    loss = criterion(y_pred, y_baho)\n",
    "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    optimizer.zero_grad() #Har bir epoch uchun grad ni 0 ga tenglashtirib olish\n",
    "    #(3.2)-->Backpropagation|||Teskari hisoblash\n",
    "    loss.backward()\n",
    "    #(3.3)--> Step||| w ning qiymatini  yangilash\n",
    "    optimizer.step()\n",
    "#Bashorat uchun qiymat||| Ushbu qiymatimiz ham tensor bo'lishi kerak    \n",
    "soat_test = torch.Tensor([[4.]])\n",
    "print(\"Bashorat (training dan keyin),  4 saot o'qilganda:\", model.forward(soat_test).data[0][0].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c8143d-8c41-44b6-ba99-7777243fa6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.6606290340423584 \n",
      "Epoch: 100 | Loss: 0.6895908117294312 \n",
      "Epoch: 200 | Loss: 0.6396538615226746 \n",
      "Epoch: 300 | Loss: 0.613638162612915 \n",
      "Epoch: 400 | Loss: 0.5898334980010986 \n",
      "Epoch: 500 | Loss: 0.5676499605178833 \n",
      "Epoch: 600 | Loss: 0.5469623804092407 \n",
      "Epoch: 700 | Loss: 0.5276616811752319 \n",
      "Epoch: 800 | Loss: 0.5096443891525269 \n",
      "Epoch: 900 | Loss: 0.49281272292137146 \n",
      "\n",
      " Trainingdan so'ng bashorat qilib ko'ramiz \n",
      "==================================================\n",
      "1 soat o'qilganda imtihondan o'ta olish: 0.4058 |  50% dan yuqori: False\n",
      "7 soat o'qilganda imtihondan o'ta olish: 0.9625 |  50% dan yuqori : True\n"
     ]
    }
   ],
   "source": [
    "#Kerakli kutubxonalarni chaqirib olish \n",
    "import torch \n",
    "#Ma'lumotlarni tensor ko'rinishida yuklab olish\n",
    "x_soat = torch.Tensor([[1.],\n",
    "                       [2.],\n",
    "                       [3.],\n",
    "                       [4.]])\n",
    "y_ikkilik = torch.Tensor([[0.],\n",
    "                       [0.],\n",
    "                       [1.],\n",
    "                       [1.]])\n",
    "\n",
    "#(1) Class yordamida model qurib olish --> \"Model\"\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        #Bu yerda nn.Module bu yerda super class(Pytorch)\n",
    "        super().__init__()\n",
    "        #torch.nn.Linear(#kirish, #chiqish) chiziqli model\n",
    "        self.linear = torch.nn.Linear(1,1) #1ta kirish & 1ta chiqish\n",
    "    #Metod yordamida to'g'ri hisoblash arxitikturasini kiritamiz(forward pass)    \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "#Bizning model\n",
    "model=Model()\n",
    "# print(model)\n",
    "#(2) Loss va optimizer larni tanlab olish \n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "#(3) Training(3.1),   Backward(3.2), Step(3.3)\n",
    "#(3.1)-->Training\n",
    "for epoch in range(1000):    #Epochlar soni 1000\n",
    "    y_pred = model(x_soat)\n",
    "    #Loss|||xatolikni hisoblash va chop qilish\n",
    "    loss = criterion(y_pred, y_ikkilik)\n",
    "    if epoch % 100 == 0:# loss ni hisoblash\n",
    "        print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
    "\n",
    "    optimizer.zero_grad() #Har bir epoch uchun grad ni 0 ga tenglashtirib olish\n",
    "    #(3.2)-->Backpropagation|||Teskari hisoblash\n",
    "    loss.backward()\n",
    "    #(3.3)--> Step||| w ning qiymatini  yangilash\n",
    "    optimizer.step()\n",
    "#Bashorat uchun qiymat||| Ushbu qiymatimiz ham tensor bo'lishi kerak    \n",
    "print(f\"\\n Trainingdan so'ng bashorat qilib ko'ramiz \\n{'=' * 50}\")\n",
    "# 1 soat uchun bashorat\n",
    "hour_var = model(torch.tensor([[1.0]]))\n",
    "print(f\"1 soat o'qilganda imtihondan o'ta olish: {hour_var.item():.4f} |  50% dan yuqori: {hour_var.item() > 0.5}\")\n",
    "# 7 soat uchun bashorat\n",
    "hour_var = model(torch.tensor([[7.0]]))\n",
    "print(f\"7 soat o'qilganda imtihondan o'ta olish: {hour_var.item():.4f} |  50% dan yuqori : { hour_var.item() > 0.5}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f8f6d0-959d-45d3-a570-e2a483587bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x_data: tensor([[-0.2941,  0.4874,  0.1803,  ...,  0.0015, -0.5312, -0.0333],\n",
      "        [-0.8824, -0.1457,  0.0820,  ..., -0.2072, -0.7669, -0.6667],\n",
      "        [-0.0588,  0.8392,  0.0492,  ..., -0.3055, -0.4927, -0.6333],\n",
      "        ...,\n",
      "        [-0.1765,  0.3769,  0.4754,  ..., -0.0462, -0.7327, -0.4000],\n",
      "        [ 0.0000,  0.2362,  0.1803,  ...,  0.0820, -0.8463,  0.0333],\n",
      "        [-0.8824,  0.0653,  0.2459,  ...,  0.1177, -0.8984, -0.8333]]) \n",
      "epoch # 0 --> Loss 0.6514\n",
      "epoch # 1000 --> Loss 0.6221\n",
      "epoch # 2000 --> Loss 0.5531\n",
      "epoch # 3000 --> Loss 0.4910\n",
      "epoch # 4000 --> Loss 0.4714\n",
      "epoch # 5000 --> Loss 0.4667\n",
      "epoch # 6000 --> Loss 0.4655\n",
      "epoch # 7000 --> Loss 0.4649\n",
      "epoch # 8000 --> Loss 0.4645\n",
      "epoch # 9000 --> Loss 0.4640\n",
      "epoch # 10000 --> Loss 0.4636\n",
      "epoch # 11000 --> Loss 0.4632\n",
      "epoch # 12000 --> Loss 0.4628\n",
      "epoch # 13000 --> Loss 0.4624\n",
      "epoch # 14000 --> Loss 0.4620\n",
      "\n",
      " Trainingdan so'ng test qilib ko'ramiz \n",
      "==================================================\n",
      " test uchun data == tensor([[ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667]])\n",
      "\n",
      " Natija quyidagicha \n",
      "==================================================\n",
      "Bashorat qiymat: 0.1714 | diabet mavjudligi: False\n"
     ]
    }
   ],
   "source": [
    "#Kerakli kutubxonalrni chaqirib olish\n",
    "import torch \n",
    "import numpy as np\n",
    "url = \"https://raw.githubusercontent.com/MansurCompAI/AI_with_PythonPytorch_Uz/master/Data/diabetes.csv\"\n",
    "#Ma'lumotlarni numpy yordamida yuklab olish\n",
    "xy_data = np.loadtxt(url, delimiter=',', dtype = np.float32)\n",
    "# x va y data larga ajratib chiqish (Traning data)\n",
    "x_data = torch.from_numpy(xy_data[:750, 0:-1])\n",
    "y_data = torch.from_numpy(xy_data[:750, [-1]])\n",
    "\n",
    "print(f' x_data: {x_data} ')\n",
    "\n",
    "# Class yordamida model qurib olish --> \"Model\"\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Chiziqli modellar\n",
    "        self.linear1 = torch.nn.Linear(8, 6) # kirish 8 va chiqish 6\n",
    "        self.linear2 = torch.nn.Linear(6, 4) # kirish 6 va chiqish 4\n",
    "        self.linear3 = torch.nn.Linear(4, 1) # kirish 4 va chiqish 1\n",
    "        # Aktivatsiya funksiyasi (Sigmoid&ReLU)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    # metod yordamida forward ni belgilash    \n",
    "    def forward(self, x):\n",
    "        natija1 = self.relu(self.linear1(x))\n",
    "        natija2 = self.relu(self.linear2(natija1))\n",
    "        y_pred = self.sigmoid(self.linear3(natija2))\n",
    "        return y_pred\n",
    "# Bizning modelimiz\n",
    "model = Model()   \n",
    "\n",
    "# Lossni va optimizer tanlash\n",
    "criterioin = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) \n",
    "\n",
    "# Training\n",
    "for epoch in range(15000):     # Epochlar soni 15000\n",
    "    y_pred = model(x_data)  # Forward (to'g'ri xisoblash)\n",
    "    loss = criterioin(y_pred, y_data)\n",
    "    if epoch % 1000 == 0:# loss ni hisoblash\n",
    "        print(f'epoch # {epoch} --> Loss {loss.item():.4f}')\n",
    "    optimizer.zero_grad()  # Optimizerni nolga tenglab olish\n",
    "    loss.backward() # Teskari hisoblash (Back prop)\n",
    "    optimizer.step()  # Optimizer orqali w ni qiymatini yangilash \n",
    "print(f\"\\n Trainingdan so'ng test qilib ko'ramiz \\n{'=' * 50}\")   \n",
    "# Testing data\n",
    "x_test = torch.from_numpy(xy_data[752:753, 0:-1])\n",
    "print(f' test uchun data == {x_test}')\n",
    "test = model(x_test)\n",
    "print(f\"\\n Natija quyidagicha \\n{'=' * 50}\")\n",
    "print(f\"Bashorat qiymat: {test.item():.4f} | diabet mavjudligi: { test.item() > 0.5}\")\n",
    "\n",
    "\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     # kirish ma'lumotlar (input)larni ajratib olish \n",
    "#     # Variables (Tensorga) larga o'girib olish\n",
    "#     inputs, labels = Variable(inputs), Variable(labels)\n",
    "    \n",
    "#     # Trainingni amalga oshirish\n",
    "  \n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "  \n",
    "# class CustomDataset(Dataset):\n",
    "    \n",
    "#     # Ma'lumotlarni tayyorlash\n",
    "#     def __init__(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         return \n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return\n",
    "\n",
    "# dataset = CustomDataset()\n",
    "# train_loader = DataLoader(dataset=dataset,\n",
    "#                           batch_size=64,\n",
    "#                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a778488-6301-4119-af51-b4d072e68b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Batch: 1 | Loss: 0.6327\n",
      "Epoch 1 | Batch: 2 | Loss: 0.5789\n",
      "Epoch 1 | Batch: 3 | Loss: 0.6320\n",
      "Epoch 1 | Batch: 4 | Loss: 0.6781\n",
      "Epoch 1 | Batch: 5 | Loss: 0.6474\n",
      "Epoch 1 | Batch: 6 | Loss: 0.6854\n",
      "Epoch 1 | Batch: 7 | Loss: 0.7014\n",
      "Epoch 1 | Batch: 8 | Loss: 0.6244\n",
      "Epoch 1 | Batch: 9 | Loss: 0.6702\n",
      "Epoch 1 | Batch: 10 | Loss: 0.6776\n",
      "Epoch 1 | Batch: 11 | Loss: 0.6542\n",
      "Epoch 1 | Batch: 12 | Loss: 0.5945\n",
      "Epoch 2 | Batch: 1 | Loss: 0.6471\n",
      "Epoch 2 | Batch: 2 | Loss: 0.6627\n",
      "Epoch 2 | Batch: 3 | Loss: 0.6011\n",
      "Epoch 2 | Batch: 4 | Loss: 0.6779\n",
      "Epoch 2 | Batch: 5 | Loss: 0.6546\n",
      "Epoch 2 | Batch: 6 | Loss: 0.6619\n",
      "Epoch 2 | Batch: 7 | Loss: 0.6478\n",
      "Epoch 2 | Batch: 8 | Loss: 0.6394\n",
      "Epoch 2 | Batch: 9 | Loss: 0.6318\n",
      "Epoch 2 | Batch: 10 | Loss: 0.6244\n",
      "Epoch 2 | Batch: 11 | Loss: 0.6544\n",
      "Epoch 2 | Batch: 12 | Loss: 0.6845\n",
      "Epoch 3 | Batch: 1 | Loss: 0.6859\n",
      "Epoch 3 | Batch: 2 | Loss: 0.6009\n",
      "Epoch 3 | Batch: 3 | Loss: 0.6393\n",
      "Epoch 3 | Batch: 4 | Loss: 0.6703\n",
      "Epoch 3 | Batch: 5 | Loss: 0.6628\n",
      "Epoch 3 | Batch: 6 | Loss: 0.7012\n",
      "Epoch 3 | Batch: 7 | Loss: 0.6619\n",
      "Epoch 3 | Batch: 8 | Loss: 0.6472\n",
      "Epoch 3 | Batch: 9 | Loss: 0.6549\n",
      "Epoch 3 | Batch: 10 | Loss: 0.5929\n",
      "Epoch 3 | Batch: 11 | Loss: 0.6314\n",
      "Epoch 3 | Batch: 12 | Loss: 0.6298\n",
      "Epoch 4 | Batch: 1 | Loss: 0.6313\n",
      "Epoch 4 | Batch: 2 | Loss: 0.6703\n",
      "Epoch 4 | Batch: 3 | Loss: 0.6389\n",
      "Epoch 4 | Batch: 4 | Loss: 0.6546\n",
      "Epoch 4 | Batch: 5 | Loss: 0.6627\n",
      "Epoch 4 | Batch: 6 | Loss: 0.6233\n",
      "Epoch 4 | Batch: 7 | Loss: 0.6155\n",
      "Epoch 4 | Batch: 8 | Loss: 0.6622\n",
      "Epoch 4 | Batch: 9 | Loss: 0.6314\n",
      "Epoch 4 | Batch: 10 | Loss: 0.6859\n",
      "Epoch 4 | Batch: 11 | Loss: 0.6318\n",
      "Epoch 4 | Batch: 12 | Loss: 0.6752\n"
     ]
    }
   ],
   "source": [
    "#Kerakli kutubxonalrni chaqirib olish\n",
    "import torch \n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ma'lumotlarni ham class yordamida tartibga keltirib DataLoaderdan foydalanamiz.\n",
    "class DiabetesDataset(Dataset):\n",
    "    \"\"\" Qandli diabet ma'lumotlar to'plami\"\"\"\n",
    "\n",
    "    # Ma'lumotlarga dastalbki ishlov berish, yuklab olish, tensorga o'girish\n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt(url,\n",
    "                        delimiter=',', dtype=np.float32)\n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True)\n",
    "                          \n",
    "# xy_data = np.loadtxt('../Data/diabetes.csv', delimiter=',', dtype = np.float32)\n",
    "# # x va y data larga ajratib chiqish (Traning data)\n",
    "# x_data = torch.from_numpy(xy_data[:750, 0:-1])\n",
    "# y_data = torch.from_numpy(xy_data[:750, [-1]])\n",
    "\n",
    "# Class yordamida model qurib olish --> \"Model\"\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Chiziqli modellar\n",
    "        self.linear1 = torch.nn.Linear(8, 6) # kirish 8 va chiqish 6\n",
    "        self.linear2 = torch.nn.Linear(6, 4) # kirish 6 va chiqish 4\n",
    "        self.linear3 = torch.nn.Linear(4, 1) # kirish 4 va chiqish 1\n",
    "        # Aktivatsiya funksiyasi (Sigmoid&ReLU)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "    # metod yordamida forward ni belgilash    \n",
    "    def forward(self, x):\n",
    "        natija1 = self.relu(self.linear1(x))\n",
    "        natija2 = self.relu(self.linear2(natija1))\n",
    "        y_pred = self.sigmoid(self.linear3(natija2))\n",
    "        return y_pred\n",
    "# Bizning modelimiz\n",
    "model = Model()   \n",
    "\n",
    "# Lossni va optimizer tanlash\n",
    "criterion = torch.nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) \n",
    "\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(4):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # kirish ma'lumotlarini ajratib olish\n",
    "        inputs, labels = data\n",
    "\n",
    "        # forward (to'g'ri xisoblash)\n",
    "        y_pred = model(inputs) \n",
    "\n",
    "        # natijalarni chop qilish\n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
    "            \n",
    "        # gradientni nolga tenglash,back propogation, w ni qiymatini yangilash.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16d69f-031d-4a72-bd79-937ab37d00b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7fec1-5a13-4d3f-a7b4-d21364293efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046bf70f-7cf1-489a-a21e-688e96c6cf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58b721-c39a-4661-bba6-a600220fb3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
